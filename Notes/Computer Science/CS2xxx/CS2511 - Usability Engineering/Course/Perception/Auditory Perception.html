<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Auditory Perception</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      div.line-block{white-space: pre-line;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="/Users/Noel/Developer/Projects/Notes/Build/note-style.css">
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="auditory-perception" class="level1">
<h1>Auditory Perception</h1>
<p>Like the visual system, the human auditory system can be divided into two stages:</p>
<ul>
<li>physical reception of sounds</li>
<li>processing and interpretation</li>
</ul>
<p>Like the visual system, the auditory system has strengths and weaknesses:</p>
<ul>
<li>certain things can’t be heard even when present</li>
<li>processing allows sounds to be constructed from incomplete information
<ul>
<li>e.g. the fundamental of a bass instrument</li>
</ul></li>
</ul>
<p>Sound has pitch, timbre, and loudness.</p>
<section id="pitch" class="level2">
<h2>Pitch</h2>
<p>We talk about JND again with pitch (just noticeable difference). This is pretty constant for pitch but varies for frequency because of the logarithmic scale.</p>
</section>
<section id="loudness" class="level2">
<h2>Loudness</h2>
<p>We adapt to loudness, so it’s not a good cue.</p>
<p>Human beings are very poor at judging the loudness of sounds that are heard for less than 0.2 seconds. These sounds seem much quieter than they are.</p>
<p>Perceived loudness varies with frequency (strongest in the centre).</p>
</section>
<section id="timbre" class="level2">
<h2>Timbre</h2>
<p>The timbre of a sound is determined by the relative level of its harmonics and by its amplitude envelope (the way in which the amplitude varies over the time). The envelope helps us distinguish between instruments with similar harmonic content.</p>
</section>
<section id="localisation-of-sound-sources" class="level2">
<h2>Localisation of Sound Sources</h2>
<ul>
<li>Stereo hearing allows us to locate the source of a sound by comparing the sound at each ear
<ul>
<li>we compare amplitude between each ear (interaural intensity)</li>
<li>also compare time of arrival (interaural delay)
<ul>
<li>we can recognise differences of 10 microseconds or less between the time of arrival of a sound at each ear</li>
</ul></li>
<li>stereo hearing works in the horizontal plane only and is least effective in the middle range of audible frequencies</li>
</ul></li>
<li>Head movement allows us to improve the localisation accuracy of stereo hearing
<ul>
<li>localisation accuracy better for non-musical sounds</li>
<li>localisation best straight ahead and straight behind
<ul>
<li>front-back reversals happen but are less common for clicks and noises</li>
</ul></li>
<li>localisation varies with frequency
<ul>
<li>good below 1000 Hz, based on timing/phase differences</li>
<li>poor between 1000 and 3000 Hz</li>
<li>good above 3000 Hz, based on intensity differences</li>
</ul></li>
</ul></li>
<li>Analysis of reflected versus direct sound yield information about the route a sound has travelled to reach us.</li>
<li>Familiarity / pattern-matching affects localisation accuracy – both ways
<ul>
<li>e.g. we see a ventriloquist’s dummy’s mouth move and assume the sound is comping from there</li>
</ul></li>
</ul>
</section>
<section id="vertical-localisation" class="level2">
<h2>Vertical Localisation</h2>
<p>Research has shown the the average listener can reliably distinguish only three vertical source locations.</p>
<section id="distance" class="level3">
<h3>Distance</h3>
<p>Judgment of distance is based partly on intensity – the quieter the sound, the further away the source.</p>
<p>Distance also affects:</p>
<ul>
<li>The audio spectrum of the sound – some frequencies travel better than others
<ul>
<li>bass frequencies don’t travel very well</li>
</ul></li>
<li>The balance between reflected and direct sound – the further the sound has travelled, the more likely it is to include a significant percentage of reflected components</li>
</ul>
</section>
<section id="head-related-transfer-functions" class="level3">
<h3>Head-Related Transfer Functions</h3>
<p>Sound localisation can be improved by tailoring the sound distribution.</p>
<p>Ideally, HRFTs should be tailored to suit the individual. However, this is complex and costly.</p>
<p>Researchers are currently trying to develop non-individualised HRTFs which will give a useful improvement in localisation accuracy for a substantial percentage of the population.</p>
</section>
</section>
<section id="sensory-memory-for-audio" class="level2">
<h2>Sensory memory for Audio</h2>
<p>As with other senses, it appears that there is a sensory memory associated with the hearing system – the echoic memory.</p>
<p>It stores the last few seconds of incoming sound, in its raw form. There’s disagreement as to how long the store is, but studies agree that there’s a store.</p>
</section>
<section id="speech-and-non-speech-sound" class="level2">
<h2>speech and Non-Speech Sound</h2>
<p>Research suggests that the human hearing system responds differently to speech than to other sound.</p>
</section>
<section id="summary" class="level2">
<h2>Summary</h2>
<p>Human beings are good at:</p>
<ul>
<li>Detecting changes in pitch, and distinguishing between differing successive pitches</li>
<li>recognising and distinguishing between rhythmic structures</li>
<li>recognising and distinguishing between familiar timbres</li>
<li>localising the source of low-pitched and high-pitched sounds in the horizontal plane</li>
</ul>
<p>We’re bad at:</p>
<ul>
<li>recognising absolute pitches, or distinguishing between different pitches presented at significantly different times</li>
<li>detecting changes in loudness (unless the changes are huge)</li>
<li>recognising and distinguishing between unfamiliar timbres</li>
<li>localising the source of mid-pitched sounds in the horizontal plane</li>
<li>localising the source of all sounds in the vertical plane</li>
</ul>
</section>
<section id="applications" class="level2">
<h2>Applications</h2>
<p>In mainstream computing, sound is rarely used as a primary means to communicate information.</p>
<p>It is used mainly for:</p>
<ul>
<li>simple warnings (success, failure, etc.)</li>
<li>to make educational applications more engaging</li>
<li>in entertainment […]</li>
</ul>
<p>[…]</p>
<p>Sound is used more extensively in a number of specialised fields, including:</p>
<ul>
<li>applications for blind and visually-impaired people</li>
<li>hands-free/eyes-free applications</li>
</ul>
<p>Sound has been used in interfaces in a number of ways. Synthetic speech is easy to use, and its meaning is immediately obvious.</p>
<p>But speech is a relatively slow method of presenting information and places a heavy load on cognitive resources.</p>
<p>Two different approaches have been developed:</p>
<ul>
<li>Auditory icons are based on natural sounds, and are intended to be instantly recognisable to the user.
<ul>
<li>However, it can be difficult to find appropriate sounds to represent many functions.</li>
</ul></li>
<li>Earcons are musical motifs, etc., which are structured so as to convey information. This overcomes the problem of associating sounds with functions, but the user has to learn the meanings of the earcons in each application.
<ul>
<li>e.g. a particular rhythm means one thing, a particular timbre means another thing</li>
</ul></li>
</ul>
<p>A newer idea is speech-earcons (Spearcons):</p>
<ul>
<li>Spearcons use speech which has been speeded-up until it is only just recognisable.
<ul>
<li>users can initially identify the meaning of a spearcon by listening carefully to the speech</li>
<li>however, as they learn the meaning of each spearcon, they can ignore the speech content and treat it as non-speech sound
<ul>
<li>this involves much less cognitive effort.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="applications-1" class="level2">
<h2>Applications</h2>
<p>TIDE Maths projects used sound to help blind people work with mathematical equations:</p>
<ul>
<li>3D sound projection was used to place each term of a polynomial expression at a unique position in space</li>
<li>To avoid overloading the user with sounds, each expression had a characteristic ‘background’ sound which it made when not selected
<ul>
<li>e.g. the same sound but muffled/mumbled</li>
</ul></li>
<li>When selected, the term would be spoken out, with non-speech sound used to indicate parentheses, grouping, etc.
<ul>
<li>e.g. tone for each parenthesis and a continuous tone inside the pair</li>
</ul></li>
</ul>
<p>Placing things in 3D space allowed users to employ spatial memory rather than relying solely on short-term memory.</p>
<p>In one implementation, the terms could be manipulated using a data glove. This further improved performance, perhaps because it allowed use of both spatial and haptic/kinaesthetic memory.</p>
</section>
</section>
</body>
</html>
