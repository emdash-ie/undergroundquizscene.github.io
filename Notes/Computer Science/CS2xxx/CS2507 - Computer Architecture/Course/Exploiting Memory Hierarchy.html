<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Exploiting Memory Hierarchy</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      div.line-block{white-space: pre-line;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="/Users/Noel/Developer/Projects/Github Page/Notes/note-style.css">
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="exploiting-memory-hierarchy-main-memory-associative-cache" class="level1">
<h1>Exploiting Memory Hierarchy: Main Memory, Associative Cache</h1>
<section id="write-allocation" class="level2">
<h2>Write Allocation</h2>
<p>[missed the start of this]</p>
<p>A write miss is when the processor wants to write data into memory that isn’t there(?).</p>
<p>The memory needs to be fetched into a buffer and then overwritten.</p>
<p>An alternative is write-around – don’t fetch the block, just write into the main memory. (no writing in the cache?)</p>
<p>For write-back:</p>
<ul>
<li><p>usually fetch the block</p></li>
<li><p>use write buffer to avoid overwrite</p>
<ul>
<li>write to the buffer, check if consistent, then write to main memory if it is</li>
</ul></li>
</ul>
<p>## Example: Intrinsity FastMATH</p>
<ul>
<li><p>embedded MIPS processor</p>
<ul>
<li><p>12-stage pipeline</p></li>
<li><p>uses one part of cache for instruction fetch and one part for data, which can operate in parallel</p>
<ul>
<li>called a split cache strategy</li>
</ul></li>
</ul></li>
</ul>
<p>The miss rate is much lower for the instruction cache (because it’s mostly reading?).</p>
<p>## Main Memory Supporting Caches</p>
<p>Use DRAMs for main memory:</p>
<ul>
<li><p>fixed width (e.g. 1 word)</p></li>
<li><p>connected by fixed-width clocked bus</p>
<ul>
<li>bus clock is typically slower than CPU clock</li>
</ul></li>
</ul>
<p>[…]</p>
</section>
<section id="performance-summary" class="level2">
<h2>Performance Summary</h2>
<p>When CPU performance is increased, the miss penalty becomes more significant.</p>
<p>When the clock rate is increased, memory stalls account for more CPU cycles (because the CPU cannot progress unless the data is brought in).</p>
<p>Can’t neglect cache behaviour when evaluating system performance.</p>
</section>
<section id="associative-caches" class="level2">
<h2>Associative Caches</h2>
<p>Fully associative – allow a given block to go in any cache entry. (place in next available spot)</p>
<p>This requires all entries to be searched at once. (the cache has to be searched for the memory address passed down from the CPU)</p>
<p>Also requires a comparator per entry, which is expensive.</p>
<ul>
<li>placing data is simple but comparing it is expensive</li>
</ul>
</section>
<section id="n-way-set-associative" class="level2">
<h2>N-way Set Associative</h2>
<ul>
<li>middle ground between fully associative and direct map</li>
</ul>
<p>Each set contains n entries. Block number determines which set. (block number modulo <code>n</code>)</p>
<p>Search all entries in a given set at once.</p>
<p>Requires n comparators (less expensive than fully associative).</p>
<ul>
<li>can view direct map as 1-way set associative</li>
</ul>
<p>## Associativity Example</p>
<ul>
<li><p>With the direct map, everything’s a miss because 0 and 8 keep replacing each other.</p></li>
<li><p>With 2-way set associative, the 6 replaces the 8 because the 8 is least recently used.</p></li>
<li><p>With fully associative, we get the maximum number of hits.</p>
<ul>
<li><p>however, it’s slower to access because we have to search the whole thing</p></li>
<li><p>also, we need the extra hardware</p></li>
</ul></li>
</ul>
<p>So we want to increase associativity to decrease the miss rate, but only up to a point. The improvement from increasing associativity decreases as you go.</p>
</section>
<section id="replacement-policy" class="level2">
<h2>Replacement Policy</h2>
<p>For direct map, there’s no choice.</p>
<p>For set associative, prefer a non-valid entry. If there isn’t one, choose among entries in the set.</p>
<p>Can also use least-recently used, which is simple for 2-way, manageable for 4-way, and too hard after that.</p>
<p>For more sets, random replacement works well.</p>
</section>
<section id="multilevel-caches" class="level2">
<h2>Multilevel Caches</h2>
<p>Primary cache (L1) is attached to the CPU and is fast but small.</p>
<p>L2 cache services misses from this – it’s bigger but slower.</p>
<p>Main memory services misses from L2.</p>
<p>Some high end systems have an extra L3 as well.</p>
<p>Adding more caches means searching is more complicated – have to perform the search at every level (in the worst case).</p>
<p>Goals:</p>
<ul>
<li><p>reduce cache miss penalty</p></li>
<li><p>close gap between fast CPU clock rate and long access time for DRAM</p></li>
</ul>
<section id="l1-vs-l2" class="level3">
<h3>L1 vs L2</h3>
<p>Primary cache:</p>
<ul>
<li>focus on minimal hit time</li>
</ul>
<p>L2 cache:</p>
<ul>
<li>focus on low miss rate to avoid main memory access</li>
<li>hit time has less overall impact</li>
</ul>
</section>
<section id="interactions-with-advanced-cpus" class="level3">
<h3>Interactions with Advanced CPUs</h3>
<p>Out-of-order CPUs can execute instructions during cache miss:</p>
<ul>
<li><p>pending store stays in load/store unit</p></li>
<li><p>dependent instructions wait in reservation stations</p>
<ul>
<li>independent instructions continue</li>
</ul></li>
</ul>
<p>Now the effect of a miss depends on program data flow. This is much harder to analyse, so we use system simulation.</p>
</section>
</section>
<section id="memory-system-dependability" class="level2">
<h2>Memory System Dependability</h2>
<ul>
<li>How much you can depend on a memory system</li>
</ul>
<p>Three things we can do with faults:</p>
<ul>
<li><p>avoid them</p></li>
<li><p>tolerate them</p></li>
<li><p>predict them (fault forecasting)</p></li>
</ul>
<p>Faults can be intermittent or permanent.</p>
<section id="dependability-measures" class="level3">
<h3>Dependability Measures</h3>
<p>You can measure reliability using the mean time to failure (MTTF).</p>
<p>You can measure the service interruption using the mean time to repair (MTTR).</p>
<p>Combining these gives the mean time between failures:</p>
<ul>
<li>MTBF = MTTF + MTTR</li>
</ul>
<p>We can define availability based on this:</p>
<ul>
<li>Availability = MTTF / (MTTF + MTTR)</li>
</ul>
<p>We can improve the availability by:</p>
<ul>
<li><p>increasing MTTF</p>
<ul>
<li>fault avoidance, fault tolerance, fault forecasting</li>
</ul></li>
<li><p>reducing MTTR</p>
<ul>
<li>improved tools and processes for diagnosis and repair</li>
</ul></li>
</ul>
</section>
<section id="the-hamming-single-error-correction-sec-code" class="level3">
<h3>The Hamming Single Error Correction (SEC) Code</h3>
<p>This is a redundancy scheme for memory. It implements fault tolerance.</p>
<p>The Hamming distance is the number of bits that are different between two bit patterns.</p>
<p>[check]</p>
<section id="encoding-sec" class="level4">
<h4>Encoding SEC</h4>
<ul>
<li>there’s a full example in the book</li>
</ul>
<p>To calculate Hamming Error Correction Code:</p>
<ul>
<li>number the bits from the left, starting at 1</li>
<li>all bit positions that are a power of 2 are parity bits</li>
<li>all other bit positions are used for data bits</li>
</ul>
<p>Then each parity bit checks certain data bits:</p>
<ul>
<li><p>p1 checks all numbers whose binary representation has a 1 at the end</p>
<ul>
<li>so 1, 3, 5, 7, …</li>
</ul></li>
<li>p2 checks all numbers whose binary representation has a 1 in the second-last place
<ul>
<li>so 2, 3, 6, 7, …</li>
</ul></li>
<li><p>p3 checks all numbers whose binary representation has a 1 in the third-last place</p>
<ul>
<li>so 4-7, 12-16, …</li>
</ul></li>
<li><p>etc. (laid out like a truth table)</p></li>
</ul>
<p>To fit a word of size 2^n, you need n+1 parity bits.</p>
<pre><code>* so 4 parity bits to represent 8 bits, meaning 12 bits total space</code></pre>
<p>To determine the value of a parity bit, count the number of 1s in the data bits it covers. If it’s odd, set the parity bit to 1, if it’s even, set it to 0.</p>
</section>
</section>
</section>
<section id="virtual-machine" class="level2">
<h2>Virtual Machine</h2>
<ul>
<li><p>host computer emulates guest operating system and machine resources</p>
<ul>
<li><p>improved isolation of multiple guests</p></li>
<li><p>avoids security and reliability problems</p></li>
<li><p>aids sharing of resources</p></li>
</ul></li>
</ul>
<p>Virtualisation has some performance impact, and so is only feasible with modern high-performance computers.</p>
<section id="virtual-machine-monitor-aka-hypervisor" class="level3">
<h3>Virtual Machine Monitor (aka Hypervisor)</h3>
<p>This is the heart of the virtual machine technology.</p>
<p>It maps virtual resources to physical resources (memory, I/O devices, CPU).</p>
<p>Guest code runs on native machine in user mode.</p>
<ul>
<li>traps to VMM on privileged instructions and access to protected resources</li>
</ul>
<p>Guest OS may be different from host OS.</p>
<p>[more here]</p>
</section>
<section id="instruction-set-support" class="level3">
<h3>Instruction Set Support</h3>
<ul>
<li>user and system processor modes</li>
</ul>
<p>Privileged instructions are only available in system mode – trap to system if executed in user mode.</p>
<p>All physical resources are only accessible using privileged instructions (e.g. page tables, interrupt controls, I/O registers).</p>
<p>Current ISAs are adapting to virtualisation (as it wasn’t prominent when they were created).</p>
</section>
</section>
<section id="virtual-memory" class="level2">
<h2>Virtual Memory</h2>
<p>Use main memory as a cache for disk storage.</p>
<ul>
<li>managed jointly by CPU hardware and the OS</li>
</ul>
<p>Programs share main memory.</p>
<ul>
<li><p>each gets a private virtual address space holding its frequently used code and data</p></li>
<li><p>protected from other programs</p></li>
</ul>
<p>The CPU and the OS translate virtual addresses to physical addresses.</p>
<ul>
<li><p>virtual memory blocks are called pages</p></li>
<li><p>virtual memory translation miss is called a page fault</p>
<ul>
<li>requires going to the hard disk to load the block into virtual memory</li>
</ul></li>
</ul>
<section id="address-translation" class="level3">
<h3>Address Translation</h3>
<ul>
<li><p>virtual memory space can have more addresses than the physical address space</p>
<ul>
<li>discrepancy handled by translation</li>
</ul></li>
</ul>
</section>
<section id="page-fault-penalty" class="level3">
<h3>Page Fault Penalty</h3>
<p>Page fault penalty for going to the disk is very high.</p>
<p>Try to minimise the page fault rate:</p>
<ul>
<li><p>fully associative placement</p>
<ul>
<li><p>no pages are in contention (until you run out of space?)</p></li>
<li><p>search time is high</p></li>
</ul></li>
<li><p>smart replacement algorithms used by the operating system to track placement</p></li>
</ul>
<p>### Page Tables</p>
<ul>
<li><p>attempt to make up for high search time from fully associative placement</p></li>
<li><p>stores placement information</p>
<ul>
<li><p>array of page table entries, indexed by virtual page number</p></li>
<li><p>page table register in CPU points to page table in physical memory</p></li>
</ul></li>
<li><p>If page is present in memory</p>
<ul>
<li><p>page table entry stores the physical page number</p></li>
<li><p>also other status bits (referenced, dirty, etc.)</p></li>
</ul></li>
<li><p>if a page is not present</p>
<ul>
<li>page table entry can refer to location in swap space on disk</li>
</ul></li>
</ul>
<p>Swap space: partition on hard drives in linux – faster to access than the rest of the drive.</p>
<section id="translation-using-a-page-table" class="level4">
<h4>Translation Using a Page Table</h4>
<p>Valid bit marks whether the page is in physical memory or on the disk.</p>
<p>Two virtual addresses can point to the same physical address (e.g. when two programs are sharing the same data).</p>
</section>
</section>
<section id="replacement-and-writes" class="level3">
<h3>Replacement and Writes</h3>
<p>To reduce the page fault rate, prefer least-recently used replacement.</p>
<ul>
<li><p>reference bit in page table set to 1 on access to page</p></li>
<li><p>periodically cleared to 0 by the OS</p></li>
<li><p>a page with reference bit = 0 has not been used recently</p></li>
</ul>
<p>A disk write takes millions of cycles. So:</p>
<ul>
<li><p>write a whole block at once, rather than individual locations</p></li>
<li><p>write through is impractical</p></li>
<li><p>use write-back when replacing a page</p></li>
<li><p>dirty bit in page table set when a page is written</p>
<ul>
<li>a modified page is often called a dirty page</li>
</ul></li>
</ul>
</section>
<section id="fast-translation-using-a-tlb-translation-look-aside-buffer" class="level3">
<h3>Fast Translation Using a TLB (Translation Look-aside Buffer)</h3>
<p>Address translation would appear to require extra memory references.</p>
<ul>
<li><p>one to access the page table entry</p></li>
<li><p>then the actual memory access</p></li>
</ul>
<p>But access to page tables has good locality.</p>
<ul>
<li><p>so use a fast cache of page table entries within the CPU</p></li>
<li><p>called TLB</p></li>
<li><p>typically 16-512 PTEs, 0.5-1 cycle for a hit, 10-100 cycles for miss, 0.01%-1% miss rate</p></li>
<li><p>misses could be handled by hardware or software</p></li>
<li><p>have to copy the valid, dirty, and reference bits from the page table</p></li>
</ul>
</section>
<section id="tlb-misses" class="level3">
<h3>TLB Misses</h3>
<p>Occurs when no entry in TLB matches a virtual address.</p>
<p>If the page is in memory, load the PTE from memory and try.</p>
<ul>
<li><p>could be handled in hardware</p>
<ul>
<li>this gets complex for more complicated page table structures</li>
</ul></li>
<li><p>or in software</p>
<ul>
<li>raise a special exception, with optimised handler</li>
</ul></li>
</ul>
<p>If the page is not in memory (page fault):</p>
<ul>
<li><p>OS handles fetching the page and updating the page table</p></li>
<li><p>then restart the faulting instruction</p></li>
</ul>
<p>### Page Fault Handler</p>
<ol type="1">
<li><p>Use faulting virtual address to find PTE.</p></li>
<li><p>Locate page on disk.</p></li>
<li><p>Choose page to replace.</p>
<ul>
<li>if dirty, write to disk first</li>
</ul></li>
<li><p>Read page into memory and update page table.</p></li>
<li><p>Make process runnable again.</p>
<ul>
<li>restart from faulting instruction</li>
</ul></li>
</ol>
</section>
<section id="example-tlb-and-cache-interaction-in-fastmath" class="level3">
<h3>Example: TLB and Cache Interaction in FastMATH</h3>
<p>If the cache tag uses physical address:</p>
<ul>
<li>need to translate (from virtual to physical) before cache lookup</li>
</ul>
<p>If the cache tag uses virtual addresses:</p>
<ul>
<li><p>complications due to aliasing</p>
<ul>
<li>different virtual addresses for shared physical address</li>
</ul></li>
</ul>
</section>
<section id="memory-protection" class="level3">
<h3>Memory Protection</h3>
<p>Different tasks can share parts of their virtual address spaces.</p>
<ul>
<li><p>but need to protect against errant access</p></li>
<li><p>requires OS assistance</p></li>
</ul>
<p>Hardware support for OS protection:</p>
<ul>
<li><p>privileged supervisor mode (aka kernel mode)</p></li>
<li><p>privileged instructions</p></li>
<li><p>page tables and other state information only accessible in supervisor mode</p></li>
<li><p>system call exception (e.g. syscall in MIPS)</p></li>
</ul>
</section>
</section>
</section>
<section id="common-framework-for-memory-hierarchy" class="level1">
<h1>Common Framework for Memory Hierarchy</h1>
<p>Common principles apply at all levels of the memory hierarchy.</p>
<ul>
<li>based on notions of caching</li>
</ul>
<p>At each level in the hierarchy, the following are relevant:</p>
<ul>
<li><p>block placement</p></li>
<li><p>finding a block</p></li>
<li><p>replacement on a miss</p></li>
<li><p>write policy</p></li>
</ul>
<section id="block-placement" class="level2">
<h2>Block Placement</h2>
<p>Determined by associativity.</p>
<ul>
<li><p>direct-mapped (1-way associative)</p>
<ul>
<li>only one choice for placement</li>
</ul></li>
<li><p>n-way set associative</p>
<ul>
<li>n choices within a set</li>
</ul></li>
<li><p>fully associative</p>
<ul>
<li>any location</li>
</ul></li>
</ul>
<p>Higher associativity reduces miss rate.</p>
<ul>
<li>increases complexity, cost, and access time</li>
</ul>
<p>## Finding a Block</p>
<p>Hardware caches:</p>
<ul>
<li>reduce the number of comparisons to reduce the cost.</li>
</ul>
<p>Virtual memory:</p>
<ul>
<li><p>full lookup table make full associativity feasible</p></li>
<li><p>benefit in reduced miss rate</p></li>
</ul>
<p>## Replacement</p>
<p>Choice of entry to replace on a miss:</p>
<ul>
<li><p>LRU</p>
<ul>
<li>complex and costly hardware for high associativity</li>
</ul></li>
<li><p>random</p>
<ul>
<li>close to LRU, easier to implement</li>
</ul></li>
</ul>
<p>[…]</p>
</section>
<section id="write-policy" class="level2">
<h2>Write Policy</h2>
<p>Write-through</p>
<ul>
<li><p>update both upper and lower levels</p></li>
<li><p>simplifies replacement, but may require write buffer</p></li>
</ul>
<p>Write-back</p>
<ul>
<li><p>update upper level only</p></li>
<li><p>update lower level when block is replaced</p></li>
<li><p>need to keep more state</p></li>
</ul>
<p>Virtual memory</p>
<ul>
<li>only write-back is feasible, given disk write latency</li>
</ul>
</section>
<section id="sources-of-misses" class="level2">
<h2>Sources of Misses</h2>
<p>Compulsory misses (aka cold start misses)</p>
<ul>
<li>first access to a block that has never been in cache</li>
</ul>
<p>Capacity misses</p>
<ul>
<li><p>due to finite cache size</p></li>
<li><p>a replaced block is later accessed again</p></li>
</ul>
<p>Conflict misses (aka collision misses)</p>
<ul>
<li><p>in a non-fully-associative cache</p></li>
<li><p>due to competition for entries in a set</p></li>
<li><p>would not occur in a fully associative cache of the same total size</p></li>
</ul>
</section>
<section id="cache-design-tradeoffs" class="level2">
<h2>Cache Design Tradeoffs</h2>
<ul>
<li><p>increase cache size</p>
<ul>
<li><p>decreases capacity misses</p></li>
<li><p>may increase access time</p></li>
</ul></li>
<li><p>increase associativity</p>
<ul>
<li><p>decrease conflict misses</p></li>
<li><p>may increase access time</p></li>
</ul></li>
<li><p>increase block size</p>
<ul>
<li><p>decreases compulsory misses</p></li>
<li><p>increases miss penalty (have to swap out the whole block)</p></li>
<li><p>for very large block size, may increase miss rate due to pollution</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="finite-state-machines" class="level1">
<h1>Finite State Machines</h1>
<p>Note: important to know how the tag and index are converted to the address for checking if you have a cache hit – important for exams.</p>
<p>## Cache Controller FSM</p>
<ul>
<li>some optimisations are possible</li>
</ul>
</section>
<section id="cache-coherence-problem" class="level1">
<h1>Cache Coherence Problem</h1>
<ul>
<li><p>suppose two CPU cores share a physical address space</p>
<ul>
<li>write-through caches</li>
</ul></li>
</ul>
<p>The memory view of the processors is based on their individual caches.</p>
<p>Each core has its own cache, but since both get their data from memory, how do we keep data in both coherent?</p>
<p>One core might write a value, and write-through will update that data in memory. This value might now be incorrect in the other core’s cache.</p>
</section>
<section id="coherence-defined" class="level1">
<h1>Coherence Defined</h1>
<ul>
<li><p>reads return the most recently written value</p></li>
<li><p>if P writes X and then reads X, the read should return the written value</p></li>
<li><p>if P1 writes X and then P2 reads X, the read should also return the written value</p></li>
<li><p>if P1 writes X and then P2 writes X, all processors should see the writes in the same order</p>
<ul>
<li>then the final value of X will be consistent</li>
</ul></li>
</ul>
<p>## Cache Coherence Protocols</p>
<ul>
<li><p>cache in multiprocessors supports</p>
<ul>
<li><p>migration of data to local caches (reduces bandwidth for shared memory)</p></li>
<li><p>replication of read-shared data (reduces contention for access)</p>
<ul>
<li>contention in main memory is part of why it’s slow – cache prevents this</li>
</ul></li>
</ul></li>
</ul>
<p>Operations performed by caches in multiprocessors to ensure coherence</p>
<pre><code>* known as cache coherence protocol

* the key part is state tracking of shared data</code></pre>
<section id="snooping-protocols" class="level3">
<h3>Snooping Protocols</h3>
<ul>
<li><p>Each cache monitors bus reads/writes</p></li>
<li><p>sharing status of blocks copied to different caches</p></li>
</ul>
</section>
<section id="invalidating-snooping-protocols" class="level3">
<h3>Invalidating Snooping Protocols</h3>
<ul>
<li><p>processor gets exclusive access to a block when it is to be written</p>
<ul>
<li><p>broadcasts an invalidate message on the bus</p></li>
<li><p>subsequent read in another cache misses</p>
<ul>
<li>owning cache supplies updated value</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="memory-consistency" class="level2">
<h2>Memory Consistency</h2>
<p>When are writes seen by other processors?</p>
<ul>
<li><p>“seen” means a read returns the written value</p>
<ul>
<li>can’t be instantaneous</li>
</ul></li>
</ul>
<p>Assumptions:</p>
<ul>
<li><p>a write completes only when all processors have seen it</p></li>
<li><p>a processor does not reorder writes with other accesses</p>
<ul>
<li>so any processor that sees a write must have seen all previous writes</li>
</ul></li>
</ul>
</section>
</section>
<section id="supporting-multiple-issue-arm-and-intel-i7" class="level1">
<h1>Supporting Multiple Issue (ARM and Intel i7)</h1>
<ul>
<li><p>Both have multi-banked caches that allow multiple accesses per cycle (assuming no back conflicts)</p></li>
<li><p>core i7 optimisation</p>
<ul>
<li><p>return requested word first</p></li>
<li><p>non-blocking cache for out-of-order processors</p>
<ul>
<li><p>hit under miss: hides miss latency</p></li>
<li><p>miss under miss: overlaps miss latency</p></li>
</ul></li>
<li><p>data pre-fetching</p>
<ul>
<li><p>looks at pattern of data misses</p></li>
<li><p>predicts next address to start fetching data before a miss occurs</p></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="pitfall" class="level1">
<h1>Pitfall</h1>
<ul>
<li><p>byte vs. word addressing</p>
<ul>
<li><p>need to be clear on this</p></li>
<li><p>example: 32-byte direct-mapped cache, 4-byte blocks</p>
<ul>
<li><p>byte 36 maps to block 1 (since 9 (= 36 / 4) modulo 8 (= 32 / 4) = 1)</p></li>
<li><p>word 36 maps to block 4 (since 36 modulo 8 = 4)</p></li>
</ul></li>
</ul></li>
<li><p>ignoring memory system effects when writing or generating code</p>
<ul>
<li><p>example: iterating over rows vs. columns of arrays</p></li>
<li><p>large strides result in poor locality</p></li>
<li><p>can drastically reduce performance</p></li>
</ul></li>
<li><p>in multiprocessor with shared L2 or L3 cache</p>
<ul>
<li><p>less associativity than cores results in conflict misses</p></li>
<li><p>more cores -&gt; need to increase associativity</p></li>
</ul></li>
<li><p>using AMAT (average memory access time) to evaluate performance of out-of-order processors</p>
<ul>
<li><p>ignores effect of non-blocking accesses (being able to do other things while waiting for memory)</p></li>
<li><p>instead, evaluate performance by simulation</p></li>
</ul></li>
<li><p>implementing a VMM on an ISA not designed for virtualisation</p>
<ul>
<li><p>e.g. non-privileged instructions accessing hardware resources</p></li>
<li><p>either extend the ISA or require the guest OS not to use problematic instructions</p></li>
</ul></li>
</ul>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<ul>
<li><p>principle of locality is important</p></li>
<li><p>fast memory is small, large memory is slow</p>
<ul>
<li>caching gives the illusion of fast, large memory</li>
</ul></li>
<li><p>memory hierarchy</p>
<ul>
<li>L1 &lt;-&gt; L2 &lt;-&gt; … &lt;-&gt; DRAM &lt;-&gt; disk</li>
</ul></li>
<li><p>memory system design is critical for multiprocessors</p>
<ul>
<li>processors are very fast, but memory is slow – good design helps get good performance out of the CPU</li>
</ul></li>
</ul>
</section>
</body>
</html>
